from typing import Union, Optional, TypeVar, Sequence, overload, Any, List

import torch
from torch.autograd import Variable
import numpy as np

VarTensor = Union['_TensorBase', 'torch.autograd.Variable']

Index = Union['torch.autograd.Variable', '_TensorBase', int, Sequence[Union[int, slice]]]
Value = Union[float, int, '_TensorBase']

T = TypeVar('T', bound='_TensorBase')

class _TensorBase:
    def __init__(self, *dims: int) -> None: ...

    @overload
    def size(self) -> Sequence[int]: ...
    @overload
    def size(self, dim: int) -> int: ...
    def size(self, dim: Optional[int] = None) -> Union[int, Sequence[int]]: ...

    @overload
    def sum(self) -> float: ...
    @overload
    def sum(self: T, dim: int, keepdim: bool = False, out: Optional[T] = None) -> T: ...
    def sum(self: T,
            dim: Optional[int] = None,
            keepdim: Optional[bool] = False,
            out: Optional[T] = None) -> Any: ...  # not sure why Any is required here

    shape: Sequence[int]

    def dim(self) -> int: ...
    def ndim(self) -> int: ...

    def __getitem__(self: T, key: Index) -> T: ...
    def __setitem__(self: T, key: Index, value: Value) -> T: ...

    # see https://github.com/python/mypy/issues/2783#issuecomment-276596902
    def __eq__(self, other: Any) -> 'ByteTensor': ...  # type: ignore
    def __lt__(self, other: Any) -> 'ByteTensor': ...  # type: ignore
    def __le__(self, other: Any) -> 'ByteTensor': ...  # type: ignore
    def __gt__(self, other: Any) -> 'ByteTensor': ...  # type: ignore
    def __ge__(self, other: Any) -> 'ByteTensor': ...  # type: ignore
    def __ne__(self, other: Any) -> 'ByteTensor': ...  # type: ignore

    def __add__(self: T, other: Value) -> T: ...
    def __radd__(self: T, other: Value) -> T: ...
    def __iadd__(self: T, other: Value) -> T: ...

    def __sub__(self: T, other: Value) -> T: ...
    def __rsub__(self: T, other: Value) -> T: ...
    def __isub__(self: T, other: Value) -> T: ...

    def __mul__(self: T, other: Value) -> T: ...
    def __rmul__(self: T, other: Value) -> T: ...
    def __imul__(self: T, other: Value) -> T: ...
    def __matmul__(self: T, other: Value) -> T: ...

    def __truediv__(self: T, other: Value) -> T: ...
    def __rtruediv__(self: T, other: Value) -> T: ...
    def __itruediv__(self: T, other: Value) -> T: ...

    def numpy(self) -> np.ndarray: ...

    def cpu(self: T) -> T: ...
    def new(self: T, *args, **kwargs) -> T: ...

    def resize_(self: T, *sizes: int) -> T: ...
    def fill_(self: T, value: float) -> T: ...

    def backward(self,
                 gradient: Optional[VarTensor] = None,
                 retain_graph: Optional[bool] = None,
                 create_graph: Optional[bool] = None) -> None: ...

    def squeeze(self: T, dim: Optional[int] = None) -> T: ...
    def unsqueeze(self: T, dim: int) -> T: ...
    def unsqueeze_(self: T, dim: int) -> T: ...

    def view(self: T, *dims: int) -> T: ...
    def expand(self: T, *dims: int) -> T: ...
    def expand_as(self: T, other: _TensorBase) -> T: ...

    def contiguous(self: T) -> T: ...

    def float(self) -> 'FloatTensor': ...
    def long(self) -> 'LongTensor': ...
    def clone(self: T) -> T: ...

    def tolist(self) -> List[float]: ...

class Tensor(_TensorBase): pass
class LongTensor(_TensorBase): pass
class IntTensor(_TensorBase): pass
class FloatTensor(_TensorBase): pass
class ByteTensor(_TensorBase): pass
